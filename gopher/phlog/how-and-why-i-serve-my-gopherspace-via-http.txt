I'm always late at discovering things. I grew up with a Commodore 64 in the early to mid '90s and didn't really know or care about the Internet at all until graphical browsers like Netscape Navigator and Internet Explorer were considered the norm. I suppose I didn't imagine I would have any use for it. After all I had hundreds of games on floppies and all the information I could possible ever need in old magazines and manuals. I completely missed out on BBS and Usenet and have barely ventured into IRC (yet).

One area of the Internet I only recently learned about is Gopher. I believe it was my destiny to come by it at the very moment I did, and in the following pages I will relate to you why. It was a sunny April afternoon and I was spending my time watching let's play videos on my main monitor and browsing imageboards on the second one. An anon on /g/ mentioned the SDF Public Access UNIX System in a thread about text-based interfaces, and I decided to check it out. Incidentally I had been planning to start writing a personal blog for some time, and joining SDF seemed like a perfect way for me to host a simple text-based site on the cheap. However as I read the FAQ I also came across this Gopher thing. Pocket gophers, commonly referred to as gophers, are burrowing rodents of the family Geomyidae. Now I should mention that I have been exceptionally close to animals on a metaphysical level for as long as I can remember. I find myself repeatedly embodying a new animal. I started out as a wombat, and even before my life began I was really drawn towards martens. Then I turned into a horse, which I still am for the most part, but as you can see it is now a peculiar horse who also happens to inhabit a gopher hole.

So I decided to have a gopherspace. But as I didn't really know what a gopherspace is, I had to do some research and soon learned that it is not uncommon to use one for blogging, with the one difference being that in the gopher world it is called "phlogging" -- a word not audibly distant from the act of phlagellation. But wait! I already had a place for my blog. What else could I use my gopherspace for? I found myself in a bit of a pickle -- until in a flash of genius I came up with the idea of placing my blog texts in the gopherspace and using the "regular" webspace to serve the same texts to people who don't use gopher. Obviously I knew I would be too lazy to manually edit both versions of the site whenever I add new content, so I needed to figure out a method of translating gopher menu items into html links and mapping the structure of the gopherspace into a similarly navigable html site to be served over http.

Incidentally, I have a confession to make. When I started writing this post I intended it to be a somewhat technical overview of what I did to achieve my goal, not quite a guide or a tutorial but nevertheless something that another fictitious human being after a similar goal could use as a leverage to get there. But as it turns out, time went by and now that I finally got back to this text I no longer precisely remember what it was that I did, what were the obstacles (although I do remember there being many), how I overcame them and which aspects of my hacked-together solution still require more or less urgent improvement.

Here's what I do remember: Redirects from the root or index.htm(l) to the dynamically generated site are done in the .htaccess file. Everything else is done with shell scripts; there is no PHP or Perl and certainly no JavaScript. The main script starts processing from the root of the gopherspace. If it finds a gophermap, it forwards it to another script that processes it line by line according to the rules of the gopher protocol (lines starting with 0 are assumed to be links to text files, lines starting with 1 links to subdirectories and so on). If no gophermap is found, all files in the directory are created links or other html elements based on their extensions. Images and sounds are embedded in the page, for example. All links include a reference to an appropriate script and file in the query string.

Originally I was going to write one big script that would generate the whole site by calling itself recursively. I'm not sure why I split it into separate scripts. I don't feel good about that.

A brief summary of what currently works: directories, text files, images, sounds. Files of all other types are treated as downloads.

What doesn't work: external links, Telnet links, full-text search, anything else not specifically mentioned as working above. A lot of that stuff I have either no use for or no convenient way of testing, or both. Once I get external http links to work (and I do plan to), I believe I've got support for everything I'm ever going to use in this phlog of mine.
